{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e837470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing different dataset approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model, X_train, X_valid, y_train, y_valid):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_vals = np.arange(50, 450, 50)\n",
    "results = {}\n",
    "for val in estimator_vals:\n",
    "    results[val] = val\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X_full = pd.read_csv('./data/housing_prices_competition/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./data/housing_prices_competition/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                  X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "print(f'Dataset shape: {X_train.shape}')\n",
    "print(f'Categorical columns: \\n {categorical_cols}')\n",
    "print(f'Numerical columns: \\n {numerical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2364f6",
   "metadata": {},
   "source": [
    "### Cross-validation scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83337f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "])\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # Multiply by -1 since sklearn calculates *negative* MAE\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=3,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace7596",
   "metadata": {},
   "source": [
    "### Data exploration for pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns that have missing values\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "cols_with_missing_data = [col for col in X_train.columns\n",
    "                          if X_train[col].isnull().any()]\n",
    "cols_with_missing_numerical_data = [col for col in X_train.columns \n",
    "                                    if X_train[col].isnull().any()\n",
    "                                    and X_train[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(missing_val_count_by_column[missing_val_count_by_column != 0])\n",
    "print(f'{cols_with_missing_numerical_data=}')\n",
    "\n",
    "# So there's missing garage data for houses with no garages. \n",
    "# Assumption: no garage is closer to a poor, old garage than to a new, good garage. \n",
    "#             so we'll input the worst metrics for the garages that don't exist.\n",
    "garage_cols = [col for col in X_train.columns\n",
    "               if 'Garage' in col]\n",
    "\n",
    "print(garage_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to handle specially \n",
    "special_cols = ['GarageYrBlt', 'LotFrontage']\n",
    "numerical_cols = [col for col in numerical_cols if col not in special_cols]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "garage_num_transformer = SimpleImputer(strategy='constant', fill_value=X_train['GarageYrBlt'].min())\n",
    "lot_frontage_num_transformer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('garage_year', garage_num_transformer, ['GarageYrBlt']),\n",
    "        ('lot_frontage', lot_frontage_num_transformer, ['LotFrontage']),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=200, criterion='poisson', random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cb6cb",
   "metadata": {},
   "source": [
    "### Train the model on the full dataset and make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X_full[my_cols].copy()\n",
    "clf.fit(X_all, y)\n",
    "preds = clf.predict(X_all)\n",
    "print('MAE:', mean_absolute_error(y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217ced2",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c838931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(f'Dataset shape: {X_train.shape}\\n')\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "cols_with_missing_data = [col for col in X_train.columns\n",
    "                          if X_train[col].isnull().any()]\n",
    "cols_with_categ_data = [col for col in X_train.columns\n",
    "                        if X_train[col].dtype == 'object']\n",
    "cols_with_safe_categ_data = [col for col in X_train.columns \n",
    "                             if set(X_valid[col]).issubset(X_train[col])]\n",
    "cols_with_bad_categ_data = list(set(cols_with_categ_data) - set(cols_with_safe_categ_data))\n",
    "\n",
    "cols_with_missing_data_percs = (missing_val_count_by_column/X_train.shape[0]*100)[missing_val_count_by_column != 0]\n",
    "cols_with_majority_missing = cols_with_missing_data_percs[cols_with_missing_data_percs > 50.0]\n",
    "\n",
    "cols_to_delete = list(set(cols_with_majority_missing.index).union(set(cols_with_bad_categ_data)))\n",
    "\n",
    "print(f'{(missing_val_count_by_column != 0).sum()}/{X_train.shape[1]} columns with missing values')\n",
    "print(f'{missing_val_count_by_column.sum()}/{X_train.size} missing data points')\n",
    "print(f'Columns with missing data: {cols_with_missing_data}')\n",
    "print(f'Columns with > 50 % missing data\\n{cols_with_majority_missing}')\n",
    "print(f'{len(cols_with_categ_data)}/{X_train.shape[1]} columns with categorical data')\n",
    "print(f'Columns with categorical data: {cols_with_categ_data}')\n",
    "print(f'Columns to delete: {cols_to_delete}')\n",
    "\n",
    "# Drop columns with > 50 % of data missing or categorical data in valid set but not in train set\n",
    "X_train = X_train.drop(columns = cols_to_delete)\n",
    "X_valid = X_valid.drop(columns = cols_to_delete)\n",
    "\n",
    "# Update the list of columns with categorical data\n",
    "cols_with_categ_data = list(set(cols_with_categ_data) - set(cols_to_delete))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3720228",
   "metadata": {},
   "source": [
    "#### Ordinal and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[cols_with_categ_data] = ordinal_encoder.fit_transform(X_train[cols_with_categ_data])\n",
    "label_X_valid[cols_with_categ_data] = ordinal_encoder.transform(X_valid[cols_with_categ_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from ordinal encoding all categorical variables: \") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d829c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "mae_list = []\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))\n",
    "    mae_list.append(mae)\n",
    "\n",
    "best_model = models[mae_list.index(min(mae_list))]\n",
    "\n",
    "# Fit best model on whole dataset\n",
    "best_model = best_model.fit(X, y)\n",
    "y_hat = best_model.predict(X_test)\n",
    "mean_error = mean_absolute_error(y_test, y_hat)\n",
    "error_frac = mean_error/y.mean()*100\n",
    "print(f'{mean_error=:,.0f}')\n",
    "print(f'{error_frac=:.0f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
